{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da872c85",
   "metadata": {},
   "source": [
    "# Multientity Recommender\n",
    "This is a proof-of-concept for a Multi-entity recommender. For 1000 popular entities of books, TV shows, and movies, 30 keywords are generated to capture semantic significance to represent the entities using the GPT Completion API. \n",
    "\n",
    "### Datasets\n",
    "* The 1000 most popular books are sourced and formatted from the Kaggle Goodreads dataset. https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks?resource=download\n",
    "\n",
    "* The TV and movies are sourced and formatted from IMDB dataset https://datasets.imdbws.com/ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e4f7f8",
   "metadata": {},
   "source": [
    "### Keywords from GPT-4 Completion API\n",
    "Keywords are generated to capture plot, theme, mood, pace, tags, demographics, actors, directors, countries, awards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08bd2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "import functools\n",
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import backoff\n",
    "from gensim.models import KeyedVectors\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, aget_embedding\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "from movierecs.any2vec import BaseWord2VecRecommender\n",
    "from pkg.pools import get_tmdb_movie_metadata_for_filters\n",
    "from pkg.models.uri import Uri\n",
    "from pkg.movie_metadata import batch_get_movie_metadata\n",
    "\n",
    "openai.organization = \"REDACTED\"\n",
    "openai.api_key = \"REDACTED\"\n",
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0ad2149024069816da4072d21bcbb0b5553326e6\n",
    "KEYWORDS_PROMPT = \"\"\"\n",
    "Your task is to generate keywords that succinctly describe the {entity_type} {entity}.\n",
    "These keywords will be used to generate an embedding vector for the entity, \\\n",
    "which will be used to calculate entity-similarities.\n",
    "The keywords should capture the following information about the entity:\n",
    "- plot, themes, mood, pace and tags\n",
    "- demographics of the types people who will love this entity\n",
    "- people involved, such as actors, directors and writers\n",
    "- countries\n",
    "- awards received\n",
    "Output a comma separated list of 30 keywords.\n",
    "\"\"\"\n",
    "\n",
    "KEYWORDS_PROMPT_HASH = hashlib.sha1(KEYWORDS_PROMPT.encode()).hexdigest()\n",
    "KEYWORDS_PROMPT_HASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5744f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URI_STRING_MAP is a dict mapping URIs to strings of their titles parsed from the metadata extracted from the datasets\n",
    "URI_STRING_MAP_PATH = os.path.join(DATA_DIR, \"uri_string_map.json\")\n",
    "with open(URI_STRING_MAP_PATH, \"r+\") as f:\n",
    "    URI_STRING_MAP = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    openai.error.RateLimitError,\n",
    "    max_time=300,\n",
    ")\n",
    "async def get_entity_keywords(uri: Uri):\n",
    "    file = f\"{uri}.txt\"\n",
    "    entity_dir = DATA_DIR / KEYWORDS_PROMPT_HASH / \"keywords\" / f\"{uri.namespace}:{uri.entity}\"\n",
    "    os.makedirs(entity_dir, exist_ok=True)\n",
    "\n",
    "    if file in os.listdir(entity_dir):\n",
    "        with open(entity_dir / file, \"r\") as f:\n",
    "            return f.read()\n",
    "    \n",
    "    completion = await openai.ChatCompletion.acreate(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": KEYWORDS_PROMPT.format(\n",
    "                    entity_type=uri.entity,\n",
    "                    entity=URI_STRING_MAP[uri],\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    keywords = completion.choices[0].message[\"content\"]\n",
    "    \n",
    "    with open(entity_dir / file, \"w\") as f:\n",
    "        f.write(keywords)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem = asyncio.BoundedSemaphore(10)\n",
    "\n",
    "async def f(uri):\n",
    "    async with sem:\n",
    "        return await get_entity_keywords(uri)\n",
    "    \n",
    "# Load domain of URIs of books, movies, TV shows. \n",
    "with open(\"domain.json\", \"r+\") as fp:\n",
    "    uris = json.load(fp)\n",
    "\n",
    "keywords = await tqdm.gather(*[f(uri) for uri in uris])\n",
    "\n",
    "URI_TO_KEYWORDS = dict(zip(uris, keywords))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a73853a5",
   "metadata": {},
   "source": [
    "### Embeddings \n",
    "Use the GPT ada model to generate embeddings of 1536 dimensions to represent the entity in the embedding space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "393511e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    openai.error.RateLimitError,\n",
    "    max_time=300,\n",
    ")\n",
    "async def get_entity_embedding(\n",
    "    uri: Uri,\n",
    "    model: str = EMBEDDING_MODEL,\n",
    ") -> list[float]:\n",
    "    file = f\"{uri}.txt\"\n",
    "    entity_dir = DATA_DIR / KEYWORDS_PROMPT_HASH / \"embedding\" / f\"{uri.namespace}:{uri.entity}\"\n",
    "    os.makedirs(entity_dir, exist_ok=True)\n",
    "\n",
    "    if file in os.listdir(entity_dir):\n",
    "        with open(entity_dir / file, \"r\") as f:\n",
    "            embedding = json.load(f)\n",
    "            return embedding\n",
    "\n",
    "    keywords = URI_TO_KEYWORDS[uri]\n",
    "    embedding = await aget_embedding(keywords, model)\n",
    "    \n",
    "    with open(entity_dir / file, \"w\") as f:\n",
    "        json.dump(embedding, f)\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "sem = asyncio.BoundedSemaphore(10)\n",
    "\n",
    "async def f(uri):\n",
    "    async with sem:\n",
    "        return await get_entity_embedding(uri)\n",
    "\n",
    "embeddings = await tqdm.gather(*[f(uri) for uri in URI_TO_KEYWORDS.keys()])\n",
    "\n",
    "URI_TO_EMBEDDING = dict(zip(URI_TO_KEYWORDS.keys(), embeddings))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7920b595",
   "metadata": {},
   "source": [
    "### Recommendations\n",
    "The embeddings are stored in a Word2Vec style model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2546a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors(vector_size=len(list(URI_TO_EMBEDDING.values())[0]))\n",
    "wv.add_vectors(\n",
    "    keys=[str(uri) for uri in URI_TO_EMBEDDING.keys()],\n",
    "    weights=list(URI_TO_EMBEDDING.values()),\n",
    ")\n",
    "wv.save(str(DATA_DIR / \"multientity-ada-sample-20230503-3000.wordvectors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40b87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = BaseWord2VecRecommender(w2v_params=str(DATA_DIR + \"/multientity-ada-sample-20230503-3000.wordvectors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c985ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.cache\n",
    "def get_embedding_memoised(query: str, model: str) -> list[float]:\n",
    "    return get_embedding(query, model)\n",
    "\n",
    "def get_recommendations(query: str | list[Uri | str], filter: list[str] = None):\n",
    "    pool = {str(k) for k in URI_STRING_MAP.keys() if k.entity in filter} if filter else {str(k) for k in URI_STRING_MAP.keys()}\n",
    "    if isinstance(query, str):\n",
    "        embedding = get_embedding_memoised(query, EMBEDDING_MODEL)\n",
    "        recs = rec.get_recommendations(query_items=[np.array(embedding)], num_items=3, pool=pool)\n",
    "    elif isinstance(query, list):\n",
    "        recs = rec.get_recommendations(query_items=[str(uri) for uri in query], num_items=3, pool=pool)\n",
    "    \n",
    "    for r in recs:\n",
    "        uri = Uri(r)\n",
    "        emoji = \"🍿\" if uri.entity == \"movie\" else \"📺\" if uri.entity == \"tv\" else \"📖\"\n",
    "\n",
    "        print(emoji, URI_STRING_MAP[uri])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea008bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Harry Potter Boxed Set  Books 1-5 (Harry Potter  #1-5) by J.K. Rowling (2014)\n",
      "📖 Harry Potter and the Half-Blood Prince (Harry Potter  #6) by J.K. Rowling (2014)\n",
      "📖 Son of a Witch (The Wicked Years  #2) by Gregory Maguire (2014)\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(\"Book that has wizards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10302b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change by Stephen R. Covey (2014)\n",
      "📖 Think and Grow Rich: The Landmark Bestseller Now Revised and Updated for the 21st Century by Napoleon Hill (2014)\n",
      "📖 Emotional Intelligence: Why It Can Matter More Than IQ by Daniel Goleman (2014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meg-kumar/miniconda3/envs/w2v/lib/python3.10/site-packages/movierecs/any2vec.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  pool = [k for k in pool if k not in keys]\n",
      "/Users/meg-kumar/miniconda3/envs/w2v/lib/python3.10/site-packages/movierecs/filters.py:26: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  candidates = [i for i in candidates if i not in queries]\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(\"personal growth\", filter=[\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d71aceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍿 Yes Man (2008)\n",
      "🍿 Stand by Me (1986)\n",
      "🍿 The Karate Kid (1984)\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(\"personal growth\", filter=[\"movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcbf0dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📺 Mr. Bean (1990)\n",
      "📺 Monty Python's Flying Circus (1969)\n",
      "📺 Blackadder (1982)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meg-kumar/miniconda3/envs/w2v/lib/python3.10/site-packages/movierecs/any2vec.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  pool = [k for k in pool if k not in keys]\n",
      "/Users/meg-kumar/miniconda3/envs/w2v/lib/python3.10/site-packages/movierecs/filters.py:26: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  candidates = [i for i in candidates if i not in queries]\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(\"hilarious slapstick comedy\", filter=[\"tv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "972d5fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Batman: Arkham Asylum - A Serious House on Serious Earth by Grant Morrison (2014)\n",
      "📖 V for Vendetta by Alan Moore (2014)\n",
      "📖 Kingdom Come by Mark Waid (2014)\n"
     ]
    }
   ],
   "source": [
    "# Books close to The Dark Knight\n",
    "get_recommendations([Uri(\"imdb:movie:tt0468569\")], filter=[\"book\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47c3bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Blink: The Power of Thinking Without Thinking by Malcolm Gladwell (2014)\n",
      "📖 Intensity by Dean Koontz (2014)\n",
      "🍿 Die Hard with a Vengeance (1995)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meg-kumar/miniconda3/envs/w2v/lib/python3.10/site-packages/movierecs/any2vec.py:85: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  pool = [k for k in pool if k not in keys]\n",
      "/Users/meg-kumar/miniconda3/envs/w2v/lib/python3.10/site-packages/movierecs/filters.py:26: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  candidates = [i for i in candidates if i not in queries]\n"
     ]
    }
   ],
   "source": [
    "get_recommendations(\"hard hitting, thinker\", filter=[\"book\", \"movie\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
